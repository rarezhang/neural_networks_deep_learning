{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Recurrent Neural Networks (RNN)\n",
    "\n",
    "This lab was built by Jon Barker, Solutions Architect, July 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is aimed at Deep Learning (DL) beginners that are interested in seeing recurrent neural networks (RNNs) in action and gaining an intuition for how they work and how they are used.  This should not be your first exposure to neural networks and DL.  If it is, stop now and go complete our Intro to DL series of labs to learn the basics.\n",
    "\n",
    "Before completing this lab, we also recommend you read the Parallel Forall blog post \"Deep Learning in a Nutshell: Sequence Learning\" - you can find it [here](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning/#more-6437).  That post gives a gentle introduction to the basic idea behind Recurrent Neural Networks (RNNs) and in this lab you will get hands-on experience with some of the RNNs described in the article.\n",
    "\n",
    "In this lab we will use both the Python and Lua programming languages.  If you are not familiar with Lua you can complete out \"Intro to Torch7\" lab to familiarize yourself with the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are RNNs?\n",
    "\n",
    "RNNs are an extension of regular artificial neural networks that add connections feeding the hidden state of the neural network back into itself - these are called recurrent connections.\n",
    "\n",
    "The reason for adding these recurrent connections is to provide the network with visibility not just of the current data sample it has been provided, but also it's previous hidden state.  In some sense, this gives the network a sequential memory of what it has seen before.  \n",
    "\n",
    "This makes RNNs applicable in situations where a sequence of data is required to make a classification decision or regression estimate.  For example, you might employ an RNN to predict your future household electricity consumption based on the time-series of instantaneous power draw or you might use an RNN to predict the next word that a person will utter given the previous ten they have said."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Binary addition\n",
    "\n",
    "**Credit**:  This first example is adapted with permission from [this](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/) excellent blog post by Andrew Trask [@iamtrask](https://twitter.com/iamtrask).\n",
    "\n",
    "Further explanation of the same example can be found in [this](https://www.youtube.com/watch?v=z61VFeALk3o) video by Geoffrey Hinton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "This toy problem is chosen to demonstrate something you can do with a RNN that you cannot do easily with a regular feed-forward neural network.  The problem is adding up two binary numbers.  For example consider the following problem:\n",
    "\n",
    "![Binary addition](files/binary_addition_small.png)\n",
    "\n",
    "If you don't know how binary addition works, read [this](http://web.math.princeton.edu/math_alive/1/Lab1/BinAdd.html) first.  \n",
    "\n",
    "## Neural network solutions\n",
    "\n",
    "We could train a regular feed-forward (fully-connected) neural network to do binary addition.  For example, we could give it the following structure:\n",
    "\n",
    "![Binary addition FFNN](files/binary_addition_FF.png)\n",
    "\n",
    "This approach has some obvious limitations though:\n",
    "\n",
    "- We have to decide in advance and fix the maximum number of digits in each binary number.\n",
    "- The knowledge learned about adding digits at the beginning of the binary numbers does not get generalized and applied to digits at the end.  We have to learn different weights describing how to add digits at all locations.\n",
    "\n",
    "The RNN we are going to train has a very similar structure to a fully-connected neural network:\n",
    "\n",
    "![Binary addition RNN](files/binary_addition_RNN.png)\n",
    "\n",
    "We have changed how the network receives input.  Instead of taking in the two whole binary numbers, we just take in one digit at a time from each number and try to predict the corresponding digit in the sum.  It is the job of the RNN to remember whether we have a carried bit from an overflow when we calculated the sum at the last digit.  \n",
    "\n",
    "We have also added connections between the hidden units themselves in addition to the connections between the hidden units and the input and output units.  These additional connections mean that the hidden activity at one time step can influence the hidden activity at the next time step in addition to the influence from the input units at the next time step.  You might ask why we don't just pass the output at the previous time-step as an additional input at the next time-step instead.  Technically you could do this, but you would be missing out on the knowledge (or memory) that the hidden layer can accumulate over multiple time-steps and you would just be influenced by the last time-step alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some dependencies\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# Fixed random seed for reproducability\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# define the derivative of the sigmoid\n",
    "# this is needed for backpropagating gradients\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a lookup table mapping integers to binary representations\n",
    "int2binary = {}\n",
    "binary_dim = 8 # The maximum length of the binary numbers we'll be adding\n",
    "# Note: we don't technically need to fix the maximum length of the binary numbers to train our RNN\n",
    "# We are just doing it for convenience of creating our dataset here\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "    \n",
    "print \"The binary representation of 53 is : \" + str(int2binary[53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "hidden_dim = 16 \n",
    "# See the Geoffrey Hinton video linked above for a great explanation why we actually only need 3 hidden units\n",
    "# We only require three hidden units in our network because of the small number of required states \n",
    "# See the Geoffrey Hinton video above for a complete explanation of this\n",
    "# We are using 16 units here to speed up training\n",
    "output_dim = 1\n",
    "\n",
    "# Randomly initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1  # Weight matrix between inputs and hidden units\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1 # Weight matrix between hidden units and output units\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1 # Recurrence weight matrix between hidden units\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate a random addition problem (a + b = c)\n",
    "a_int = np.random.randint(largest_number/2) # int version\n",
    "a = int2binary[a_int] # binary encoding\n",
    "print \"a = \" + str(a) + \"\\n\"\n",
    "\n",
    "b_int = np.random.randint(largest_number/2) # int version\n",
    "b = int2binary[b_int] # binary encoding\n",
    "print \"b = \" + str(b) + \"\\n\"\n",
    "\n",
    "# Directly calculate the true answer as our label\n",
    "c_int = a_int + b_int\n",
    "c = int2binary[c_int]\n",
    "print \"c (Truth) = \" + str(c) + \"\\n\"\n",
    "    \n",
    "# This is where we'll store our (binary encoded) prediction\n",
    "d = np.zeros_like(c)\n",
    "\n",
    "layer_1_values = list()\n",
    "layer_1_values.append(np.zeros(hidden_dim))\n",
    "\n",
    "for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "print \"d (Prediction) = \" + str(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN training\n",
    "\n",
    "The algorithm we will use to train our RNN is the same stochastic gradient descent (SGD) algorithm you have seen in training multi-layer perceptrons and convolutional neural networks.  The only difference is in the backpropagation step that computes the weight updates for our slightly more complex network structure.  Study this excellent animation from [@iamtrask's blog](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/) of an RNN being trained on a four step sequence (note that the network architecture is slightly different from our example, but the training process is the same):\n",
    "\n",
    "![Backpropagation through time (BPTT)](files/BPTT.gif)\n",
    "\n",
    "As information propagates from the input neurons to the hidden neurons over the four timesteps you see that the hidden units become a combined representation of the four inputs.  Once all of the timesteps have been received the output neurons turn black, meaning that the predicted output digits are generated.  The output neurons then turn yellow, meaning that the error in those predictions is calculated.  Then comes the crucial part:  starting at the last output neuron the error gradient is computed and backpropagated to the hidden units for that time-step (the neurons go orange) and this process is also repeated for each of the previous time-steps in order.  Remember when the gradients backpropagate to the hidden units they are coming from both the output neurons and the hidden units one step ahead in the sequence.  We call this process Backpropagation Through Time (BPTT).\n",
    "\n",
    "Notice that this diagram clearly shows that you can think of an RNN that has been \"unrolled\" back through all the time steps it has processed simply as a regular feed-forward neural network trained with backpropagation but with a slightly more complicated network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Finally) the training code\n",
    "\n",
    "Now work through the code for training the RNN.  Do not worry too much about understanding all the details, but try to understand the overall flow of a training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training algorithm parameters\n",
    "alpha = 0.1   # This is the learning rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training loop - 10000 iterations\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a random addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # Directly calculate the true answer as our label\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # This is where we'll store our (binary encoded) prediction\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # Feed-forward iterating along the digits in the binary encodings\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    # BPTT starting at the end of the binary encodings and iterating backwards\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) \n",
    "                         + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "    # SGD update equations\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 500 == 0):\n",
    "        print \"Error:\" + str(overallError)\n",
    "        print \"Pred:\" + str(d)\n",
    "        print \"True:\" + str(c)\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        if a_int + b_int == out:\n",
    "            correct = \"TRUE\"\n",
    "        else:\n",
    "            correct = \"FALSE\"\n",
    "        print str(a_int) + \" + \" + str(b_int) + \" = \" + str(out) + \" \" + correct\n",
    "        print \"------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the network very quickly learns to correctly add the binary numbers in this simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part of the lab we will explore the challenges of training RNNs for real-world datasets as some of the advances that have been made in addressing these challenges.  To open Part 2 of the lab click [here](Introduction%20to%20RNNs%20-%20notebook%202%20of%202.ipynb#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
